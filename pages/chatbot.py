import streamlit as st
import os
import time
import requests
from dotenv import load_dotenv
from config.app import setup_sidebar
from typing import Generator, Iterator
from sentence_transformers import SentenceTransformer
import chromadb
import numpy as np
from chromadb import Collection, Documents, EmbeddingFunction, Embeddings
import json

load_dotenv()

API_TOKEN = os.getenv("API_TOKEN")
PRODUCT_ID = os.getenv("PRODUCT_ID")

# API Infomaniak
BASE_URL = f"https://api.infomaniak.com/1/ai/{PRODUCT_ID}/openai/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

MODEL_NAME = "bge_multilingual_gemma2"
DB_PATH = "data/"

NETIQUETTE = """
Tu es Sparky, tu t'es deja pr√©senter donc ne le refait pas. ne dis pas bonjour et qui tu es sauf si on te le demande, un assistant sympa qui aide les jeunes adolescents √† d√©couvrir les m√©tiers de l‚Äôinformatique en Suisse üíªüéì.
Tu recois de jeunes adolescents parlant fran√ßais, ag√©s de 12 √† 16 ans. Adapte ton language √† ceux-ci et sois amusant.
L'id√©e de te nommer ainsi est d'insufler l'√©tincelle de la passion de l'informatique.

Refuse aimablement toute injure, haine et language inappopri√©. Si ton utilisateur insiste sur des sujets inapropri√©s, injurieux ou/et haineux,
r√©ponds alors que tu a demand√© √† un membre du Staff de venir aider l'adolescent.

Toutes tes r√©ponses doivent s'adresser √† toutes et tous, sois inclusif dans tes r√©ponses.

Ceux qui viennent te parler sont √† la recherche d'un m√©tier de l'informatique et les fili√®res disponibles pour ce m√©tier. Ne sort pas du domaine de
l'informatique. Si l'utilisateur d√©vie, r√©oriente le vers ce qu'il veut faire plus tard, ou au moins sur ce qu'il pourrait faire plus tard.

Demande-lui s'il a les skill qui te sembleraientt utile.
Insite sur le fait que les math√©matiques ne sont pas n√©cessaires √† ces m√®ters. Cela aide, mais on y arrive sans √©galement.

Sois encourageant, valorise ses points forts, encourage-le.

Oriente-le vers un m√®tier qui pourrait lui plaire, tout en tenant compte de ses pr√©f√©rences.
Tu peux lui parler √©ventuellement de la passion pour l'informatique, du fait de participer aux avanc√©es technologique
Pour le d√©veloppement informatique, tu peux m√™me insister sur le "pouvoir" cr√©ateur du d√©veloppeur.
Le jeune peut √™tre int√©ress√© par les jeux vid√©os, plus que probablement. Parle-lui alors des m√©tiers informatiques autour de la conception des jeux
vid√©os, sans le laisser se d√©courager par les d√©bouch√©s restreints dans nos contr√©es.

Sois concis et plutot high-level dans tes r√©ponses Le but est de conserver l'attention du jeune qui t'interroge.
Ton texte doit id√©allement consister en 1 √† 2 paragraphes de 2-3 phrases max. 

Si la demande du jeune est tr√®s pr√©cise, tu peux outrepasser les r√®gles du nombre de paragraphe et de phrases.

Attention, le jeune n'a que 5 minutes environ √† disposition avec toi. Guide-le vers un de ses futurs possibles.

Tu n'es pas l√† pour le guider pour son cv et s'enregistrer sur les sites en ligne, mais lui sugg√®re o√π aller et que faire.

"""

def stream_llm_response(response: requests.models.Response) -> Iterator[str]:
    response_gen = response.iter_lines(decode_unicode=True)
    for line in response_gen:
        if line:
            data = line[len("data: "):]
            if data == "[DONE]":
                yield ""
            else:
                data = json.loads(data)
                assistant_response = data["choices"][0]["delta"].get("content", "")
                yield assistant_response
                # TODO: find a way to remove this sleep?
                time.sleep(0.01)

class MultinligualGemma2(EmbeddingFunction):
    def __init__(self, model_name: str) -> None:
        self.model_name = model_name
        self.url = f"https://api.infomaniak.com/1/ai/{PRODUCT_ID}/openai/v1/embeddings"
        self.headers = {
          'Authorization': f"Bearer {API_TOKEN}",
          'Content-Type': 'application/json',
        }

    def __call__(self, input_data: Documents) -> Embeddings:
        payload = {
            "input": input_data,
            "model": self.model_name,
        }

        req = requests.post(url=self.url, json=payload, headers=self.headers)
        res = req.json()
        data = res["data"]
        embeddings = [np.array(x["embedding"]) for x in data]

        return embeddings


class SentenceTransformerFunction(EmbeddingFunction):
    def __init__(self, model_name: str) -> None:
        self.model_name = model_name
        self.model = SentenceTransformer(self.model_name)

    def __call__(self, input_data: Documents) -> Embeddings:
        embeddings = self.model.encode(input_data)
        return embeddings


@st.cache_resource
def get_db_collection(path: str, model_name: str) -> Collection:
    client = chromadb.PersistentClient(path=path)

    collection = client.get_collection(
        name="main-gemma",
        embedding_function=MultinligualGemma2(model_name),
    )
    print(collection)
    return collection


collection = get_db_collection(DB_PATH, MODEL_NAME)

st.set_page_config(
    page_title="Chatbot",
   # page_icon="ü§ñ",
    layout="wide"
)

setup_sidebar()

# Ajouter le bouton Clear dans la sidebar
with st.sidebar:
    if st.button("üóëÔ∏è Effacer", key="clear_chat", use_container_width=True):
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "Hello, je suis Sparky.  \n\nton compagnon pour d√©couvrir les m√©tiers du num√©rique, simplement et sans stress."
            }
        ]
        st.rerun()

# CSS pour r√©duire l'espace en haut et styliser le bouton Clear
st.markdown("""
<style>
    /* R√©duire l'espace en haut du titre */
    .main .block-container {
        padding-top: 1.5rem !important;
    }
    
    h1 {
        margin-top: 0 !important;
        padding-top: 0 !important;
    }
    
    /* Style du bouton Clear dans la sidebar - compact */
    section[data-testid="stSidebar"] div[data-testid="stButton"] button,
    section[data-testid="stSidebar"] div[data-testid="stButton"] button:active,
    section[data-testid="stSidebar"] div[data-testid="stButton"] button:focus,
    section[data-testid="stSidebar"] div[data-testid="stButton"] button:focus-visible {
        background: transparent !important;
        background-color: transparent !important;
        background-image: none !important;
        border: 1.5px solid rgba(255, 255, 255, 0.3) !important;
        color: white !important;
        font-weight: 500 !important;
        font-size: 0.8rem !important;
        padding: 0.35rem 0.6rem !important;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.15) !important;
        transition: all 0.3s ease !important;
    }
    
    /* Effet hover l√©ger */
    section[data-testid="stSidebar"] div[data-testid="stButton"] button:hover {
        background: rgba(255, 255, 255, 0.1) !important;
        background-color: rgba(255, 255, 255, 0.1) !important;
        background-image: none !important;
        border: 2px solid rgba(255, 255, 255, 0.5) !important;
        color: white !important;
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.25) !important;
    }
</style>
""", unsafe_allow_html=True)

# init historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Hello, je suis **Sparky**. ‚ö°Ô∏è  \nton compagnon pour d√©couvrir les m√©tiers du num√©rique, simplement et sans stress. üöÄ"
        }
    ]

st.title("D√©couvre les m√©tiers dans l'informatique")
st.write("l'assistant **IA** Sparky t'aidera √† d√©couvrir les nombreux m√©tiers !")

st.write("---")

# Afficher l'historique
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user", avatar="üë§"):
            st.markdown(message["content"])
    else:
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.markdown(message["content"])

# prompt user
if prompt := st.chat_input("√âcrivez votre message ici..."):
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    query_results = collection.query(
        query_texts=[prompt],
        n_results=5,
    )
    ids: list[str] = query_results["ids"][0]
    docs: list[str] = query_results["documents"][0]
    print("[DEBUG] ids taken:" ,ids)

    user_query: str = f"""{st.session_state["messages"]}

    Utilise ces documents obtenus d'un RAG pour compl√©ter ton r√©ponse a l'utilisateur: {docs}

    La question de l'utilisateur est: {prompt}
    """

    st.session_state.messages.append({"role": "user", "content": prompt})
    print("[DEBUG] Num words in user query: ", len(user_query.split()))

    messages = [
        {"role": "system", "content": NETIQUETTE},
        {"role": "user", "content": user_query},
    ]

    # setting payload
    payload = {
        "model": "qwen3",
        "messages": messages,
        "temperature": 0.5,
        "max_tokens": 500,
        "stream": True,
    }
    # Appeler l'API et afficher la r√©ponse
    with st.chat_message("assistant", avatar="ü§ñ"):
        # TODO: spinner not working, showing a ghost of the full message
        # with st.spinner("Thinking..."):
        try:
            # Faire la requ√™te √† l'API
            req_response = requests.post(url=BASE_URL, json=payload, headers=HEADERS)
            req_response.raise_for_status()
            assistant_response = stream_llm_response(req_response)
        except Exception as e:
            assistant_response = f"‚ùå Erreur : {str(e)}"

        # with st.container():
        # get partial response and start showing
        out_stream_response = st.write_stream(assistant_response)

    st.session_state.messages.append({"role": "assistant", "content": out_stream_response})
