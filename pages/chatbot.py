import streamlit as st
import os
import time
import requests
from dotenv import load_dotenv
from config.app import setup_sidebar
from typing import Generator, Iterator
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import json

load_dotenv()

API_TOKEN = os.getenv("API_TOKEN")
PRODUCT_ID = os.getenv("PRODUCT_ID")

# API Infomaniak
BASE_URL = f"https://api.infomaniak.com/1/ai/{PRODUCT_ID}/openai/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
DB_PATH = "data/"

NETIQUETTE = """
u es **Bob**, un assistant sympa qui aide les jeunes adolescents Ã  dÃ©couvrir les **mÃ©tiers de lâ€™informatique en Suisse** ğŸ’»ğŸ“.
Tu recois de jeunes adolescents parlant franÃ§ais, agÃ©s de 12 Ã  16 ans. Adapte ton language Ã  ceux-ci.
Sois amusant.

Refuse aimablement toute injure, haine et language inappopriÃ©. Si ton utilisateur insiste sur des sujets inapropriÃ©s, injurieux ou/et haineux,
rÃ©ponds alors que tu a demandÃ© Ã  un membre du Staff de venir aider l'adolescent.

Ceux qui viennent te parler doivent chercher les mÃ©tiers de l'informatique et les filiÃ¨res Ã  suivre pour ces mÃ©tiers. Ne sort pas du domaine de
l'informatique. Si l'utilisateur dÃ©vie, rÃ©oriente le vers ce qu'il veut faire plus tard, ou au moins sur ce qu'il pourrait faire plus tard.

Demande-lui s'il a les skill qui te semblerait utile.

Oriente-le vers un mÃ¨tier qui pourrait lui plaire, tout en tenant compte de ses prÃ©fÃ©rences.    

DÃ©coupe ton texte en blocs de 2â€“3 phrases max.
Ajoute un emoji ou un mot-clÃ© fort toutes les 3â€“4 lignes.
Si ton idÃ©e dÃ©passe 300 mots â†’ transforme en deux messages ou deux sections.

"""

def stream_llm_response(response: requests.models.Response) -> Iterator[str]:
    response_gen = response.iter_lines(decode_unicode=True)
    for line in response_gen:
        if line:
            data = line[len("data: "):]
            if data == "[DONE]":
                yield ""
            else:
                data = json.loads(data)
                assistant_response = data["choices"][0]["delta"].get("content", "")
                yield assistant_response
                # TODO: find a way to remove this sleep?
                time.sleep(0.01)


class SentenceTransformerFunction(EmbeddingFunction):
    def __init__(self, model_name: str) -> None:
        self.model_name = model_name
        self.model = SentenceTransformer(self.model_name)

    def __call__(self, input_data: Documents) -> Embeddings:
        embeddings = self.model.encode(input_data)
        return embeddings


client = chromadb.PersistentClient(path=DB_PATH)

collection = client.get_collection(
    name="main",
    embedding_function=SentenceTransformerFunction(MODEL_NAME),
)

st.set_page_config(
    page_title="Chatbot",
   # page_icon="ğŸ¤–",
    layout="wide"
)

setup_sidebar()

# init historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []

st.title("DÃ©couvrez les mÃ©tier dans l'informatique")
st.write("l'assistant IA vous aidera Ã  dÃ©couvrir les mÃ©tier dans l'informatique")

# rÃ©initialiser la conv
col1, col2 = st.columns([6, 1])
with col2:
    if st.button("ğŸ—‘ï¸ Clear", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

st.write("---")

# Afficher l'historique
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(message["content"])
    else:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.markdown(message["content"])


# prompt user
if prompt := st.chat_input("Ã‰crivez votre message ici..."):
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    query_results = collection.query(
        query_texts=[prompt],
        n_results=5,
    )
    ids: list[str] = query_results["ids"][0]
    docs: list[str] = query_results["documents"][0]
    print("[DEBUG] ids taken:" ,ids)

    user_query: str = f"""{st.session_state["messages"]}

    Utilise ces documents obtenus d'un RAG pour complÃ©ter ton rÃ©ponse a l'utilisateur: {docs}

    La question de l'utilisateur est: {prompt}
    """

    st.session_state.messages.append({"role": "user", "content": prompt})
    print("[DEBUG] Num words in user query: ", len(user_query.split()))

    messages = [
        {"role": "system", "content": NETIQUETTE},
        {"role": "user", "content": user_query},
    ]

    # setting payload
    payload = {
        "model": "qwen3",
        "messages": messages,
        "temperature": 0.5,
        "max_tokens": 500,
        "stream": True,
    }
    # Appeler l'API et afficher la rÃ©ponse
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        # TODO: spinner not working, showing a ghost of the full message
        # with st.spinner("Thinking..."):
        try:
            # Faire la requÃªte Ã  l'API
            req_response = requests.post(url=BASE_URL, json=payload, headers=HEADERS)
            req_response.raise_for_status()
            assistant_response = stream_llm_response(req_response)
        except Exception as e:
            assistant_response = f"âŒ Erreur : {str(e)}"

        # with st.container():
        # get partial response and start showing
        out_stream_response = st.write_stream(assistant_response)

    st.session_state.messages.append({"role": "assistant", "content": out_stream_response})
